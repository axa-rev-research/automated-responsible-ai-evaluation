{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(path_name, file_name, result):\n",
    "    \"\"\"\n",
    "    Save the result to a file.\n",
    "    \n",
    "    Args:\n",
    "        path_name (str): Path to save the file\n",
    "        file_name (str): Name of the file\n",
    "        result (str): Content to save\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(path_name, f\"{file_name}.md\")\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(result)\n",
    "        \n",
    "def setup_environment():\n",
    "    \"\"\"Set up the environment variables.\"\"\"\n",
    "    try:\n",
    "        hf_key = os.environ[\"LLAMA3_KEY\"]\n",
    "    except KeyError:\n",
    "        print(\"Please set the environment variable LLAMA3_KEY\")\n",
    "        hf_key = input(\"Enter your HuggingFace API key: \")\n",
    "    return hf_key\n",
    "\n",
    "def load_model(hf_key):\n",
    "    \"\"\"Load the LLM model and tokenizer.\"\"\"\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_key)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_key, quantization_config=quant_config, device_map=\"auto\", max_length=8192)\n",
    "    config = AutoConfig.from_pretrained(model_name, token=hf_key)\n",
    "    \n",
    "    return model, tokenizer, config\n",
    "\n",
    "def generate_response(model, tokenizer, messages, config):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=4500\n",
    "    )\n",
    "\n",
    "    print(f\"Tokens used: {len(model_inputs[0])} out of {config.max_position_embeddings}\")\n",
    "\n",
    "    response = generated_ids[0][model_inputs.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step includes downloading the LLM from Hugging Face and setting up the API key.\n",
    "\n",
    "You can temporarily add your API key by using Python with os.environ[\"LLAMA3_KEY\"]=\"YOUR_API_KEY\"\n",
    "\n",
    "WARNING: NEVER SHARE/PUSH YOUR API KEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_key = setup_environment()\n",
    "model, tokenizer, config = load_model(hf_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set file directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CWD to the directory of your repository\n",
    "os.chdir('/home/gunes/fair/responsible-ai-model-cards/Steps-v3-thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card_template_path = \"files/templates/model-card.md\"\n",
    "model_card_template = open(model_card_template_path, \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual case to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_name = \"demo\"\n",
    "\n",
    "file_path = \"files/cases/\" +  case_name + \"/description.md\"\n",
    "case_description = open(file_path, \"r\").read()\n",
    "\n",
    "analysis_path = \"files/cases/\" + case_name + \"/ethics-analysis.md\"\n",
    "ethics_analysis = open(analysis_path, \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start Model Card Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in model cards and responsible AI. Your task is to complete the provided model card template using the model description and data given. For the Ethical and Legal Analysis section of the card, utilize the provided analysis. Base all content solely on the provided description, and only fill in the sections available in the template without adding new fields. Ensure that the final output is the filled model card, with no additional comments or remarks.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Template to fill: {model_card_template} Description: {case_description} Ethical and Legal Analysis: {ethics_analysis}\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_response(model, tokenizer, messages, config)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_path = \"files/cases/\" + case_name + \"/\"\n",
    "save_result(case_path, \"model-card\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
