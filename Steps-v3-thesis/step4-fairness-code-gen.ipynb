{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nbformat\n",
    "from nbformat.v4 import new_code_cell\n",
    "from nbconvert.preprocessors import ExecutePreprocessor, CellExecutionError\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_log(text, log_path):\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "    \n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(text + \"\\n\")\n",
    "        \n",
    "def extract_python_code(text):\n",
    "    # Regular expression to match Python code blocks\n",
    "    match = re.search(r'(?i)```(?:python)?\\s*([\\s\\S]*?)```', text)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract the Python code from the matched block\n",
    "        return match.group(1).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "def extract_section(file_path, title):\n",
    "    # Add the markdown header symbol to the title\n",
    "    title_header = f\"# {title}\"\n",
    "    \n",
    "    # Read the file contents\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Use regex to find the section for the given title\n",
    "    pattern = rf'{title_header}\\s*(.*?)(?=\\n#|\\Z)'  # Match content between titles\n",
    "    match = re.search(pattern, content, re.S)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()  # Return the matched content\n",
    "    else:\n",
    "        return None  # Return None if the title was not found\n",
    "    \n",
    "def open_notebook(notebook_path):\n",
    "    with open(notebook_path, 'r') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    # Extract code cells\n",
    "    code_cells = [cell['source'] for cell in notebook.cells if cell['cell_type'] == 'code']\n",
    "\n",
    "    # Combine all code into a single script\n",
    "    notebook_code = \"\\n\".join(code_cells)\n",
    "    \n",
    "    return notebook_code\n",
    "\n",
    "def add_code_cell_to_notebook(notebook_path, generated_code):\n",
    "\n",
    "    # Load the existing notebook\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "    # Create a new code cell\n",
    "    new_cell = new_code_cell(source=generated_code)\n",
    "\n",
    "    # Insert the new cell at the beginning of the notebook\n",
    "    notebook.cells.append(new_cell)\n",
    "\n",
    "    # Save the modified notebook back to the file\n",
    "    with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)\n",
    "        \n",
    "def setup_environment():\n",
    "    \"\"\"Set up the environment variables.\"\"\"\n",
    "    try:\n",
    "        hf_key = os.environ[\"LLAMA3_KEY\"]\n",
    "    except KeyError:\n",
    "        print(\"Please set the environment variable LLAMA3_KEY\")\n",
    "        hf_key = input(\"Enter your HuggingFace API key: \")\n",
    "    return hf_key\n",
    "\n",
    "def load_model(hf_key):\n",
    "    \"\"\"Load the LLM model and tokenizer.\"\"\"\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_key)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_key, quantization_config=quant_config, device_map=\"auto\", max_length=8192)\n",
    "    config = AutoConfig.from_pretrained(model_name, token=hf_key)\n",
    "    \n",
    "    return model, tokenizer, config\n",
    "\n",
    "def generate_response(model, tokenizer, messages, config):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=4500\n",
    "    )\n",
    "\n",
    "    print(f\"Tokens used: {len(model_inputs[0])} out of {config.max_position_embeddings}\")\n",
    "\n",
    "    response = generated_ids[0][model_inputs.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_notebook(notebook_path, return_last_two=False):\n",
    "    \"\"\"\n",
    "    Execute a Jupyter notebook and return the output of the last cell, \n",
    "    or the last two cells if specified.\n",
    "\n",
    "    Parameters:\n",
    "    - notebook_path: Path to the notebook file.\n",
    "    - return_last_two: If True, returns the output of the last two code cells. \n",
    "      If False, returns only the output of the last cell.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the outputs of the second last and last cells. \n",
    "      If return_last_two is False, the second element will be None.\n",
    "    \"\"\"\n",
    "    # Load the notebook\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    # Create an ExecutePreprocessor\n",
    "    ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "    \n",
    "    # Function to extract output from a cell\n",
    "    def extract_output(cell):\n",
    "        outputs = []\n",
    "        if 'outputs' in cell:\n",
    "            for output in cell['outputs']:\n",
    "                if 'text' in output:\n",
    "                    outputs.append(output['text'])\n",
    "                elif 'data' in output:\n",
    "                    if 'text/plain' in output['data']:\n",
    "                        outputs.append(output['data']['text/plain'])\n",
    "                    else:\n",
    "                        outputs.append(str(output['data']))\n",
    "        return '\\n'.join(outputs)\n",
    "    \n",
    "    # Execute all cells\n",
    "    ep.preprocess(nb, {'metadata': nb.metadata})\n",
    "    \n",
    "    # Get outputs of the last two code cells\n",
    "    code_cells = [cell for cell in nb.cells if cell.cell_type == 'code']\n",
    "    second_last_cell_output = None\n",
    "    last_cell_output = None\n",
    "\n",
    "    if len(code_cells) >= 2:\n",
    "        second_last_cell_output = extract_output(code_cells[-2])\n",
    "        last_cell_output = extract_output(code_cells[-1])\n",
    "    elif len(code_cells) == 1:\n",
    "        last_cell_output = extract_output(code_cells[-1])\n",
    "\n",
    "    # Display the outputs based on the return_last_two flag\n",
    "    if return_last_two and second_last_cell_output:\n",
    "        print(\"Output of the second last cell:\")\n",
    "        print(second_last_cell_output)\n",
    "        print(\"\\n\")\n",
    "    elif not return_last_two:\n",
    "        second_last_cell_output = None  # Do not return the second last cell's output if not needed\n",
    "    \n",
    "    print(\"Output of the last cell:\")\n",
    "    print(last_cell_output)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return second_last_cell_output, last_cell_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step includes downloading the LLM from Hugging Face and setting up the API key.\n",
    "\n",
    "You can temporarily add your API key by using Python with os.environ[\"LLAMA3_KEY\"]=\"YOUR_API_KEY\"\n",
    "\n",
    "WARNING: NEVER SHARE/PUSH YOUR API KEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_key = setup_environment()\n",
    "model, tokenizer, config = load_model(hf_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Set file directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CWD to the directory of your repository\n",
    "os.chdir('/home/gunes/fair/responsible-ai-model-cards/Steps-v3-thesis')\n",
    "\n",
    "### START - UPDATE THE FOLLOWING VARIABLES ###\n",
    "case_name = \"norauto\"\n",
    "sensitive_attribute = \"Male\"\n",
    "fairness_metric = \"Equalized Odds Difference\"\n",
    "\n",
    "mitigation_technique = \"Exponentiated Gradient\"\n",
    "constraint = \"Equalized Odds\"\n",
    "mitigation_method = f\"{mitigation_technique} with {constraint} constraint\"\n",
    "\n",
    "### END - UPDATE THE FOLLOWING VARIABLES ###\n",
    "\n",
    "measurement_code = extract_section(\"files/fairness-tools/fairlearn-lib.md\", fairness_metric)\n",
    "mitigation_code = extract_section(\"files/fairness-tools/fairlearn-lib.md\", mitigation_technique)\n",
    "\n",
    "case_path = \"files/cases/\" + case_name\n",
    "model_notebook_path = \"files/model-generator/\" + case_name + \"/model.ipynb\"\n",
    "log_path = \"files/logs/\" + case_name + \".txt\"\n",
    "\n",
    "notebook_code = open_notebook(model_notebook_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fairness measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"\"\"You are an expert in Python and the fairlearn library. Your task is to generate code that measures the fairness metric: {fairness_metric} for the model and the sensitive attribute {sensitive_attribute} using the fairlearn library. The generated code will be an addition to the existing model code and will run together seamlessly.\n",
    "    Follow these guidelines strictly and then generate the code:\n",
    "    1. Refer to the provided notebook model code for variable names.\n",
    "    2. Pay attention to function parameters, DataFrame shapes, and test variables.\n",
    "    3. Pay attention to case sensitivity in the variable names.\n",
    "    4. Extract the sensitive attribute from the variable that contains the features of the test dataset by using the name of the sensitive attribute.\n",
    "    5. Make sure to import the necessary methods from the fairlearn library.\n",
    "    6. Include comments in the generated code to explain each step clearly.\n",
    "    7. Avoid duplicating completed steps from the model code.\n",
    "    8. Avoid assuming the order of the features in the DataFrame.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Fairlearn library: {measurement_code}\\n\\nModel code: {notebook_code}\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_measurement = generate_response(model, tokenizer, messages, config)\n",
    "generated_code_measurement = extract_python_code(result_measurement)\n",
    "print(generated_code_measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate generated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"\"\"You are an intelligent assistant tasked with verifying the correctness of Python code. Your objective is to evaluate the provided generated code that is intended to integrate with the existing model code. Ensure that the generated code aligns perfectly with the model code and correctly utilizes the fairlearn library. \n",
    "        Follow these steps for the verification process and then start evaluating the code:\n",
    "        1. Make sure that sensitive attribute is extracted from the variable that contains the features of the test dataset by using the name of the sensitive attribute. Only setting the sensitive attribute as a variable is not enough.\n",
    "        2. Pay attention to case sensitivity in the variable names.\n",
    "        3. Compare variable names and structures in the generated code with those in the existing model code to ensure variable consistency.\n",
    "        4. Identify and report any hallucinations and assumptions (eg. index value of a feature) in the generated code that do not fit the context of the model code.\n",
    "        5. Make sure that necessary methods are imported from the fairlearn library.\n",
    "        6. List any necessary changes to align the generated code with the existing model code. If no update is needed, skip this step.\n",
    "        \n",
    "        If any update is needed, provide an updated and corrected version of the generated code. If no update is needed, print the generated code as is.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Here is the existing model code:\\n\\n{notebook_code} Here is the generated code to be evaluated:\\n\\n{generated_code_measurement}\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_validation_measurement = generate_response(model, tokenizer, messages, config)\n",
    "validated_code_measurement = extract_python_code(response_validation_measurement)\n",
    "print(validated_code_measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Update the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_code_cell_to_notebook(model_notebook_path, validated_code_measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Execute the updated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_last_output = execute_notebook(model_notebook_path)\n",
    "print(measurement_last_output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_code = open_notebook(model_notebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": f\"\"\"You are an expert in Python and the fairlearn library. You successfully executed the model code below and obtained the value for {measurement_last_output}. You now aim to apply the mitigation method: {mitigation_method} from the fairlearn library. The generated code will be an addition to the existing code and will run together seamlessly.\n",
    "    Follow these guidelines strictly and then generate the code:\n",
    "    1. **Comment on the Fairness State:**\n",
    "        - Provide a short description of the {fairness_metric} metric. Explain what this metric measures in general terms and what different values indicate about model fairness.\n",
    "        - Comment specifically on the measured value {measurement_last_output} that you obtained. Explain what this specific measured value indicates about the model's fairness in this context.\n",
    "        \n",
    "    2. **Generate Mitigation Code:**\n",
    "    - Generate code for the specified mitigation method by referring to the relevant mitigation method definition.\n",
    "    - Make sure to import the necessary methods from the fairlearn library. Constraint values do not take parameters.\n",
    "    - Ensure the code is compatible with the given model code and the requested mitigation method (and its constraints, if applicable).\n",
    "    - Extract the sensitive attribute from the appropriate variable in the model code.\n",
    "    - Use the appropriate variable names for the sensitive attribute, dataset, and test data as defined in the provided model code.\n",
    "    - Include code to measure the performance metrics (accuracy, precision, etc.) and the fairness metric measurement that were used previously used.\n",
    "    - Include comments in the code to explain the steps.\n",
    "\n",
    "    Organize your output into two sections:\n",
    "    1. **Comment on the fairness metric value obtained and code explanation.**\n",
    "    2. **Mitigation Code:**\n",
    "    ```python\n",
    "    # Write the mitigation code and the model performance metrics and fairness metric measurement code here\n",
    "    ```\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Model code:\\n{notebook_code}\\n\\nMitigation method definition:\\n{mitigation_code}\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mitigation = generate_response(model, tokenizer, messages, config)\n",
    "print(result_mitigation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_code_mitigation = extract_python_code(result_mitigation)\n",
    "print(generated_code_mitigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate generated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"\"\"You are an intelligent assistant tasked with verifying the correctness of Python code. Your objective is to evaluate the provided generated code that is intended to integrate with the existing model code. Ensure that the generated code aligns perfectly with the model code and correctly utilizes the fairlearn library. \n",
    "        Follow these steps for the verification process and then start evaluating the code:\n",
    "        1. Make sure that sensitive attribute is extracted from the variable that contains the features of the test dataset by using the name of the sensitive attribute. Only setting the sensitive attribute as a variable is not enough.\n",
    "        2. Pay attention to case sensitivity in the variable names.\n",
    "        3. Compare variable names and structures in the generated code with those in the existing model code to ensure variable consistency.\n",
    "        4. Identify and report any hallucinations and assumptions (eg. index value of a feature) in the generated code that do not fit the context of the model code.\n",
    "        5. Make sure that necessary methods are imported from the fairlearn library.\n",
    "        6. List any necessary changes to align the generated code with the existing model code. If no update is needed, skip this step.\n",
    "        \n",
    "        If any update is needed, provide an updated and corrected version of the generated code. If no update is needed, print the generated code as is.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Here is the existing model code:\\n\\n{notebook_code}\\n\\nHere is the generated code to be evaluated:\\n\\n{generated_code_mitigation}\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_validation_mitigation = generate_response(model, tokenizer, messages, config)\n",
    "print(response_validation_mitigation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_code_mitigation = extract_python_code(response_validation_mitigation)\n",
    "print(validated_code_mitigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Update the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_code_cell_to_notebook(model_notebook_path, validated_code_mitigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Execute the updated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigation_last_output = execute_notebook(model_notebook_path)\n",
    "print(mitigation_last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"{fairness_metric} is measured as {mitigation_last_output[0]}\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text += f\"\\n{mitigation_method} is applied and the post-mitigation performance is measured as {mitigation_last_output[1]}\"\n",
    "add_to_log(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
