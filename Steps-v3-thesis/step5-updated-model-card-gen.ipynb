{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(path_name, file_name, result):\n",
    "    \"\"\"\n",
    "    Save the result to a file.\n",
    "    \n",
    "    Args:\n",
    "        path_name (str): Path to save the file\n",
    "        file_name (str): Name of the file\n",
    "        result (str): Content to save\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(path_name, f\"{file_name}.md\")\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(result)\n",
    "        \n",
    "def setup_environment():\n",
    "    \"\"\"Set up the environment variables.\"\"\"\n",
    "    try:\n",
    "        hf_key = os.environ[\"LLAMA3_KEY\"]\n",
    "    except KeyError:\n",
    "        print(\"Please set the environment variable LLAMA3_KEY\")\n",
    "        hf_key = input(\"Enter your HuggingFace API key: \")\n",
    "    return hf_key\n",
    "\n",
    "def load_model(hf_key):\n",
    "    \"\"\"Load the LLM model and tokenizer.\"\"\"\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_key)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_key, quantization_config=quant_config, device_map=\"auto\", max_length=8192)\n",
    "    config = AutoConfig.from_pretrained(model_name, token=hf_key)\n",
    "    \n",
    "    return model, tokenizer, config\n",
    "\n",
    "def generate_response(model, tokenizer, messages, config):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=4500\n",
    "    )\n",
    "\n",
    "    print(f\"Tokens used: {len(model_inputs[0])} out of {config.max_position_embeddings}\")\n",
    "\n",
    "    response = generated_ids[0][model_inputs.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step includes downloading the LLM from Hugging Face and setting up the API key.\n",
    "\n",
    "You can temporarily add your API key by using Python with os.environ[\"LLAMA3_KEY\"]=\"YOUR_API_KEY\"\n",
    "\n",
    "WARNING: NEVER SHARE/PUSH YOUR API KEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_key = setup_environment()\n",
    "model, tokenizer, config = load_model(hf_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set file directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CWD to the directory of your repository\n",
    "os.chdir('/home/gunes/fair/responsible-ai-model-cards/Steps-v3-thesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual case to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_name = \"norauto\"\n",
    "\n",
    "model_card_path = \"files/cases/\" + case_name + \"/model-card.md\" \n",
    "model_card = open(model_card_path, \"r\").read()\n",
    "\n",
    "analysis_path = \"files/cases/\" + case_name + \"/fairness-analysis.md\"\n",
    "analysis = open(analysis_path, \"r\").read()\n",
    "\n",
    "log_path = \"files/logs/\" + case_name + \".txt\"\n",
    "logs = open(log_path, \"r\").read()\n",
    "\n",
    "updated_model_card_path = \"files/model-cards-final/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start Model Card Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"\"\"You are a specialist in model cards and responsible AI. Your task is to analyze the provided model card, measurement and mitigation methods, and intervention logs. Based on this analysis, you need to update the model card by adding a new section titled \"Fairness Interventions\".\n",
    "\n",
    "The following files will be provided:\n",
    "- **Model Card**: The current model card to be updated.\n",
    "- **Measurement and Mitigation Methods**: This file includes the rationale for the selection of fairness measurement and mitigation methods.\n",
    "- **Intervention Logs**: This file contains the results of the applied fairness interventions, including measurement results of fairness metrics and the overall performance of the mitigated model.\n",
    "\n",
    "Update the model card by adding the \"Fairness Interventions\" section as outlined below. Focus on enhancing the \"Fairness Interventions\" section without altering other parts of the model card.\n",
    "### Model Card Update Structure:\n",
    "- **Existing Sections**: Print all current sections of the model card as they are.\n",
    "- **Fairness Interventions**:\n",
    "    - **Measurement and Mitigation Methods**:\n",
    "      [Summarize the fairness measurement and mitigation methods used, including their justification.]\n",
    "    - **New Performance**:\n",
    "      [Provide the pre-intervention fairness, the post-intervention performance and the post-intervention fairness values.]\n",
    "    - **Analysis**:\n",
    "      [Analyze the results of the interventions, noting significant changes and their implications.]\n",
    "### \n",
    "Only provide the final output without any additional comments or remarks.\"\"\"\n",
    "},\n",
    "{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": f\"\"\"Model card to be updated: {model_card}\n",
    "Measurement and mitigation method decision: {analysis}\n",
    "Interventions made and results: {logs}\"\"\"\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_response(model, tokenizer, messages, config)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card_name = case_name + \"-model-card-updated\"\n",
    "save_result(updated_model_card_path, model_card_name, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
