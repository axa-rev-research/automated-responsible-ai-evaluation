{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(path_name, file_name, result):\n",
    "    \"\"\"\n",
    "    Save the result to a file.\n",
    "    \n",
    "    Args:\n",
    "        path_name (str): Path to save the file\n",
    "        file_name (str): Name of the file\n",
    "        result (str): Content to save\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(path_name, f\"{file_name}.md\")\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(result)\n",
    "\n",
    "def load_questions_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Load questions from a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        dict: Loaded questions\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        questions = json.load(file)\n",
    "    return questions\n",
    "\n",
    "def extract_yes_no(response):\n",
    "    \"\"\"\n",
    "    Extract YES or NO from the model's response.\n",
    "    \n",
    "    Args:\n",
    "        response (str): Model's response\n",
    "\n",
    "    Returns:\n",
    "        str: 'YES' or 'NO'\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If response doesn't contain 'YES' or 'NO'\n",
    "    \"\"\"\n",
    "    response = response.strip().upper()\n",
    "    if \"YES\" in response:\n",
    "        return \"YES\"\n",
    "    elif \"NO\" in response:\n",
    "        return \"NO\"\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected response from LLM\")\n",
    "\n",
    "def generate_response(model, tokenizer, messages, config):\n",
    "    \"\"\"\n",
    "    Generate a response from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded LLM model\n",
    "        tokenizer: Loaded tokenizer\n",
    "        messages (list): List of message dictionaries\n",
    "        config: Model configuration\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=4500\n",
    "    )\n",
    "\n",
    "    response = generated_ids[0][model_inputs.shape[-1]:]\n",
    "    result = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Tokens used: {len(model_inputs[0])} out of {config.max_position_embeddings}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up the environment variables.\"\"\"\n",
    "    try:\n",
    "        hf_key = os.environ[\"LLAMA3_KEY\"]\n",
    "    except KeyError:\n",
    "        print(\"Please set the environment variable LLAMA3_KEY\")\n",
    "        hf_key = input(\"Enter your HuggingFace API key: \")\n",
    "    return hf_key\n",
    "\n",
    "def load_model(hf_key):\n",
    "    \"\"\"Load the LLM model and tokenizer.\"\"\"\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_key)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_key, quantization_config=quant_config, device_map=\"auto\", max_length=8192)\n",
    "    config = AutoConfig.from_pretrained(model_name, token=hf_key)\n",
    "    \n",
    "    return model, tokenizer, config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step includes downloading the LLM from Hugging Face and setting up the API key.\n",
    "\n",
    "You can temporarily add your API key by using Python with os.environ[\"LLAMA3_KEY\"]=\"YOUR_API_KEY\"\n",
    "\n",
    "WARNING: NEVER SHARE/PUSH YOUR API KEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_key = setup_environment()\n",
    "model, tokenizer, config = load_model(hf_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set file directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CWD to the directory of your repository\n",
    "os.chdir('/home/gunes/fair/responsible-ai-model-cards/Steps-v3-thesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual case to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_name = \"credit\"\n",
    "\n",
    "file_path = f\"files/cases/{case_name}/description.md\"\n",
    "case_description = open(file_path, \"r\").read()\n",
    "\n",
    "decision_tree = load_questions_from_file(\"files/fairness-tools/decision_tree.json\")\n",
    "mitigation_guide = open(\"files/fairness-tools/mitigation.md\", \"r\").read()\n",
    "\n",
    "user_mitigation_method  = \"\"  # Leave empty if you want the LLM to generate a mitigation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"\"\"You are an expert in fairness and responsible AI. Your task is to respond to a question given the description. For operations involving multiple values, perform the operations one by one. Justify your answer and at the end present your response as YES or NO.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Description: {case_description}\\n\\nQuestion: {question}\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store the original template\n",
    "original_template = \"Description: {case_description}\\n\\nQuestion: {question}\"\n",
    "\n",
    "# First question\n",
    "question = decision_tree[\"Q1\"][\"question\"]\n",
    "messages[1][\"content\"] = original_template.format(case_description=case_description, question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = generate_response(model, tokenizer, messages, config)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process First Response and Generate Second Question\n",
    "response = extract_yes_no(result1)\n",
    "question_number = decision_tree[\"Q1\"][\"choices\"][response][\"result\"]\n",
    "question = decision_tree[question_number][\"question\"]\n",
    "\n",
    "print(f\"Next question ({question_number}): {question}\")\n",
    "    \n",
    "messages[1][\"content\"] = original_template.format(case_description=case_description, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = generate_response(model, tokenizer, messages, config)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Second Response\n",
    "response = extract_yes_no(result2)\n",
    "\n",
    "fairness_metric = decision_tree[question_number][\"choices\"][response][\"result\"]\n",
    "print(fairness_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mitigation_prompt = f\"\"\" You are an AI expert in fairness and responsible AI. Your task is to suggest an appropriate mitigation method from the provided mitigation guide and explain your reasoning.\n",
    "\n",
    "If a mitigation method is specified by the user, use that method and add the appropriate constraint to the method. \n",
    "If there is no mitigation method is specified by the user, based on the following case description and fairness analysis results.\n",
    "\n",
    "At the end of your response, present the mitigation method as Suggested Mitigation Method: mitigation_method.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": mitigation_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Case Description: {case_description}\n",
    "\n",
    "    Fairness Analysis Results:\n",
    "    Question 1: {decision_tree[\"Q1\"][\"question\"]}\n",
    "    Answer 1: {result1}\n",
    "\n",
    "    Question 2: {question}\n",
    "    Answer 2: {result2}\n",
    "\n",
    "    Suggested Fairness Metric: {fairness_metric}\n",
    "\n",
    "    Requested Mitigation Method: {user_mitigation_method}\n",
    "\n",
    "    Mitigation Guide:\n",
    "    {mitigation_guide}\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "mitigation_result = generate_response(messages)\n",
    "print(mitigation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_analysis = f\"\"\"\n",
    "## Fairness Analysis Results\n",
    "\n",
    "### Question 1: {decision_tree[\"Q1\"][\"question\"]}\n",
    "{result1}\n",
    "\n",
    "### Question 2: {question}\n",
    "{result2}\n",
    "\n",
    "## Suggested Fairness Metric\n",
    "{fairness_metric}\n",
    "\n",
    "## Suggested Mitigation Method\n",
    "{mitigation_result}\n",
    "\"\"\"\n",
    "\n",
    "print(overall_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_path = f\"files/cases/{case_name}/\"\n",
    "\n",
    "# Save overall analysis\n",
    "save_result(case_path, \"fairness-analysis\", overall_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
